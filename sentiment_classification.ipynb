{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Goal of the project**","metadata":{"id":"4oPVhM1s3z26"}},{"cell_type":"markdown","source":"One fundamental problem in sentiment analysis is categorization of sentiment polarity. Given a piece of written text, the problem is to categorize the text into one specific sentiment polarity, positive or negative (or neutral).\n\nIn this project, we aim to tackle the problem of sentiment polarity categorization.","metadata":{"id":"yQhibEwa5HdD"}},{"cell_type":"markdown","source":"**Load the packages**","metadata":{"id":"p0I5LSlWSBYT"}},{"cell_type":"code","source":"# Importing libraries\nimport pandas as pd\nimport numpy as np","metadata":{"id":"bsoN3IQ2jtDf","execution":{"iopub.status.busy":"2022-08-15T08:47:30.980742Z","iopub.execute_input":"2022-08-15T08:47:30.982169Z","iopub.status.idle":"2022-08-15T08:47:30.988229Z","shell.execute_reply.started":"2022-08-15T08:47:30.982118Z","shell.execute_reply":"2022-08-15T08:47:30.986927Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"pd.options.mode.chained_assignment = None","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:47:31.010902Z","iopub.execute_input":"2022-08-15T08:47:31.011311Z","iopub.status.idle":"2022-08-15T08:47:31.017015Z","shell.execute_reply.started":"2022-08-15T08:47:31.011280Z","shell.execute_reply":"2022-08-15T08:47:31.015925Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"markdown","source":"**Load the data**","metadata":{"id":"jRBfRrTJ36QX"}},{"cell_type":"markdown","source":"Data used in this project is [a set of product reviews](https://www.kaggle.com/datasets/nicapotato/womens-ecommerce-clothing-reviews) collected from an ecommerce business. ","metadata":{}},{"cell_type":"code","source":"# Load dataset\ndf = pd.read_csv('../input/womens-ecommerce-clothing-reviews/Womens Clothing E-Commerce Reviews.csv').iloc[: , 2:]","metadata":{"id":"sPsj8oF1rBZT","execution":{"iopub.status.busy":"2022-08-15T08:47:31.042435Z","iopub.execute_input":"2022-08-15T08:47:31.043539Z","iopub.status.idle":"2022-08-15T08:47:31.176417Z","shell.execute_reply.started":"2022-08-15T08:47:31.043460Z","shell.execute_reply":"2022-08-15T08:47:31.174868Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"# Rename Pandas columns to lower case\ndf.columns = df.columns.str.lower()","metadata":{"id":"aWfsF1AZ_3gO","execution":{"iopub.status.busy":"2022-08-15T08:47:31.178758Z","iopub.execute_input":"2022-08-15T08:47:31.179148Z","iopub.status.idle":"2022-08-15T08:47:31.184614Z","shell.execute_reply.started":"2022-08-15T08:47:31.179113Z","shell.execute_reply":"2022-08-15T08:47:31.183785Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"# Examine the data\ndf.head()","metadata":{"id":"22O6NajbrdUJ","outputId":"905ae984-5002-429b-e2a1-b51c758176bb","execution":{"iopub.status.busy":"2022-08-15T08:47:31.185790Z","iopub.execute_input":"2022-08-15T08:47:31.186366Z","iopub.status.idle":"2022-08-15T08:47:31.207664Z","shell.execute_reply.started":"2022-08-15T08:47:31.186335Z","shell.execute_reply":"2022-08-15T08:47:31.206657Z"},"trusted":true},"execution_count":101,"outputs":[{"execution_count":101,"output_type":"execute_result","data":{"text/plain":"   age                    title  \\\n0   33                      NaN   \n1   34                      NaN   \n2   60  Some major design flaws   \n3   50         My favorite buy!   \n4   47         Flattering shirt   \n\n                                         review text  rating  recommended ind  \\\n0  Absolutely wonderful - silky and sexy and comf...       4                1   \n1  Love this dress!  it's sooo pretty.  i happene...       5                1   \n2  I had such high hopes for this dress and reall...       3                0   \n3  I love, love, love this jumpsuit. it's fun, fl...       5                1   \n4  This shirt is very flattering to all due to th...       5                1   \n\n   positive feedback count   division name department name class name  \n0                        0       Initmates        Intimate  Intimates  \n1                        4         General         Dresses    Dresses  \n2                        0         General         Dresses    Dresses  \n3                        0  General Petite         Bottoms      Pants  \n4                        6         General            Tops    Blouses  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>title</th>\n      <th>review text</th>\n      <th>rating</th>\n      <th>recommended ind</th>\n      <th>positive feedback count</th>\n      <th>division name</th>\n      <th>department name</th>\n      <th>class name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>33</td>\n      <td>NaN</td>\n      <td>Absolutely wonderful - silky and sexy and comf...</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Initmates</td>\n      <td>Intimate</td>\n      <td>Intimates</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>34</td>\n      <td>NaN</td>\n      <td>Love this dress!  it's sooo pretty.  i happene...</td>\n      <td>5</td>\n      <td>1</td>\n      <td>4</td>\n      <td>General</td>\n      <td>Dresses</td>\n      <td>Dresses</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>60</td>\n      <td>Some major design flaws</td>\n      <td>I had such high hopes for this dress and reall...</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>General</td>\n      <td>Dresses</td>\n      <td>Dresses</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>50</td>\n      <td>My favorite buy!</td>\n      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>General Petite</td>\n      <td>Bottoms</td>\n      <td>Pants</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>47</td>\n      <td>Flattering shirt</td>\n      <td>This shirt is very flattering to all due to th...</td>\n      <td>5</td>\n      <td>1</td>\n      <td>6</td>\n      <td>General</td>\n      <td>Tops</td>\n      <td>Blouses</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Overview of all variables, their datatypes\ndf.info()","metadata":{"id":"S76_ZIrarfyS","outputId":"0f59097f-a3f9-4e45-8dd1-60993b14f813","execution":{"iopub.status.busy":"2022-08-15T08:47:31.210814Z","iopub.execute_input":"2022-08-15T08:47:31.211627Z","iopub.status.idle":"2022-08-15T08:47:31.239014Z","shell.execute_reply.started":"2022-08-15T08:47:31.211550Z","shell.execute_reply":"2022-08-15T08:47:31.238021Z"},"trusted":true},"execution_count":102,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 23486 entries, 0 to 23485\nData columns (total 9 columns):\n #   Column                   Non-Null Count  Dtype \n---  ------                   --------------  ----- \n 0   age                      23486 non-null  int64 \n 1   title                    19676 non-null  object\n 2   review text              22641 non-null  object\n 3   rating                   23486 non-null  int64 \n 4   recommended ind          23486 non-null  int64 \n 5   positive feedback count  23486 non-null  int64 \n 6   division name            23472 non-null  object\n 7   department name          23472 non-null  object\n 8   class name               23472 non-null  object\ndtypes: int64(4), object(5)\nmemory usage: 1.6+ MB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Identify review sentiment**","metadata":{"id":"2oQ8sTxF_tBx"}},{"cell_type":"markdown","source":"In fact, since we have star ratings, thereâ€™s arguably little need to engineer any additional sentiment-related features. All we really need to know is whether the text from a given review was positive or negative (or neutral), and we can see that from the star rating the customer has given.","metadata":{"id":"ZfM51ydLlpql"}},{"cell_type":"code","source":"star_rating = {'poor_rating': 3, 'great_rating': 4}\n\ndf['customer sentiment'] = 'Neutral'\ndf['customer sentiment'].loc[df['rating'] < star_rating['poor_rating']] = 'Negative'\ndf['customer sentiment'].loc[df['rating'] >= star_rating['great_rating']] = 'Positive'","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:47:31.241008Z","iopub.execute_input":"2022-08-15T08:47:31.241906Z","iopub.status.idle":"2022-08-15T08:47:31.254442Z","shell.execute_reply.started":"2022-08-15T08:47:31.241859Z","shell.execute_reply":"2022-08-15T08:47:31.252925Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"markdown","source":"**Define the target variable**","metadata":{}},{"cell_type":"code","source":"# Replace target variables\ndf['customer sentiment'] = df['customer sentiment'].replace(['Negative', 'Neutral', 'Positive'], [-1, 0, 1])","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:47:31.256804Z","iopub.execute_input":"2022-08-15T08:47:31.257548Z","iopub.status.idle":"2022-08-15T08:47:31.272967Z","shell.execute_reply.started":"2022-08-15T08:47:31.257497Z","shell.execute_reply":"2022-08-15T08:47:31.271292Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"markdown","source":"**Check for missing values**","metadata":{"id":"-hichKI076ug"}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"id":"173zwVrtI5tD","outputId":"79e162c7-b654-44f0-a968-9057c4962967","execution":{"iopub.status.busy":"2022-08-15T08:47:31.274354Z","iopub.execute_input":"2022-08-15T08:47:31.274756Z","iopub.status.idle":"2022-08-15T08:47:31.294269Z","shell.execute_reply.started":"2022-08-15T08:47:31.274721Z","shell.execute_reply":"2022-08-15T08:47:31.292775Z"},"trusted":true},"execution_count":105,"outputs":[{"execution_count":105,"output_type":"execute_result","data":{"text/plain":"age                           0\ntitle                      3810\nreview text                 845\nrating                        0\nrecommended ind               0\npositive feedback count       0\ndivision name                14\ndepartment name              14\nclass name                   14\ncustomer sentiment            0\ndtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"Any NaN values for the title and review text fields are filled with blank strings.","metadata":{}},{"cell_type":"code","source":"df['title'] = df['title'].fillna('')","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:47:31.350291Z","iopub.execute_input":"2022-08-15T08:47:31.350983Z","iopub.status.idle":"2022-08-15T08:47:31.359095Z","shell.execute_reply.started":"2022-08-15T08:47:31.350945Z","shell.execute_reply":"2022-08-15T08:47:31.357856Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"df['review text'] = df['review text'].fillna('')","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:47:31.382025Z","iopub.execute_input":"2022-08-15T08:47:31.383233Z","iopub.status.idle":"2022-08-15T08:47:31.393746Z","shell.execute_reply.started":"2022-08-15T08:47:31.383173Z","shell.execute_reply":"2022-08-15T08:47:31.392582Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"markdown","source":"**Concatenate the text into a single column**","metadata":{}},{"cell_type":"code","source":"df['all text'] = df['title'] + ' ' + df['review text']","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:47:31.413741Z","iopub.execute_input":"2022-08-15T08:47:31.414540Z","iopub.status.idle":"2022-08-15T08:47:31.435008Z","shell.execute_reply.started":"2022-08-15T08:47:31.414466Z","shell.execute_reply":"2022-08-15T08:47:31.433594Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"# Drop unnecessary columns\ndf.drop(['title', 'review text'], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:47:31.437886Z","iopub.execute_input":"2022-08-15T08:47:31.438660Z","iopub.status.idle":"2022-08-15T08:47:31.455332Z","shell.execute_reply.started":"2022-08-15T08:47:31.438604Z","shell.execute_reply":"2022-08-15T08:47:31.453861Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"markdown","source":"**Text Preprocessing**","metadata":{"id":"t-JHegUiAmyx"}},{"cell_type":"markdown","source":"We will need to preprocess our text to remove misleading junk and noise in order to get the best results from our model.","metadata":{"id":"qAaEbKhuYN2Z"}},{"cell_type":"code","source":"import unicodedata\nimport re\nimport nltk\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom nltk.corpus import stopwords","metadata":{"id":"G_1vIZX3LvU4","execution":{"iopub.status.busy":"2022-08-15T08:47:31.462401Z","iopub.execute_input":"2022-08-15T08:47:31.462817Z","iopub.status.idle":"2022-08-15T08:47:31.468302Z","shell.execute_reply.started":"2022-08-15T08:47:31.462785Z","shell.execute_reply":"2022-08-15T08:47:31.466968Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"nltk.download('stopwords')","metadata":{"id":"0PltzWieL0lS","outputId":"0bdfb0b1-1705-4f77-8c1e-6fc20d8f4583","execution":{"iopub.status.busy":"2022-08-15T08:47:31.491361Z","iopub.execute_input":"2022-08-15T08:47:31.492155Z","iopub.status.idle":"2022-08-15T08:47:31.499555Z","shell.execute_reply.started":"2022-08-15T08:47:31.492110Z","shell.execute_reply":"2022-08-15T08:47:31.498401Z"},"trusted":true},"execution_count":111,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"execution_count":111,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"stop_words = set(stopwords.words('english'))","metadata":{"id":"2fTnwfXEi6ci","execution":{"iopub.status.busy":"2022-08-15T08:47:31.509074Z","iopub.execute_input":"2022-08-15T08:47:31.509718Z","iopub.status.idle":"2022-08-15T08:47:31.515218Z","shell.execute_reply.started":"2022-08-15T08:47:31.509684Z","shell.execute_reply":"2022-08-15T08:47:31.513963Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"def text_cleaning(text_data):\n\n  # Remove accented characters\n  text_data = unicodedata.normalize('NFKD', text_data).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n\n  # Case conversion\n  text_data = text_data.lower()\n\n  # Remove special characters\n  text_data = re.sub(r\"[^a-zA-Z]+\", ' ', text_data)\n\n  # Text as string objects\n  text_data = str(text_data)\n\n  # Tokenization\n  tokenizer = ToktokTokenizer()\n  text_data = tokenizer.tokenize(text_data)\n\n  # Removing stopwords\n  text_data = [item for item in text_data if item not in stop_words]\n  \n  # Convert list of tokens to string data type\n  text_data = ' '.join (text_data)\n\n  return text_data","metadata":{"id":"lkxCXQhN3zVL","execution":{"iopub.status.busy":"2022-08-15T08:47:31.539047Z","iopub.execute_input":"2022-08-15T08:47:31.539988Z","iopub.status.idle":"2022-08-15T08:47:31.547150Z","shell.execute_reply.started":"2022-08-15T08:47:31.539952Z","shell.execute_reply":"2022-08-15T08:47:31.545912Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"df['all text'] = df['all text'].apply(text_cleaning)","metadata":{"id":"EAwzIcaBMExF","execution":{"iopub.status.busy":"2022-08-15T08:47:31.557856Z","iopub.execute_input":"2022-08-15T08:47:31.558686Z","iopub.status.idle":"2022-08-15T08:47:34.643008Z","shell.execute_reply.started":"2022-08-15T08:47:31.558645Z","shell.execute_reply":"2022-08-15T08:47:34.641697Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"markdown","source":"**Define X and y**","metadata":{}},{"cell_type":"markdown","source":"Now weâ€™ve examined our data, we need to create a dataset to train the model and one to hold back for testing. The aim of the model is to predict our target variable customer sentiment from the set of features X. The first step is therefore to define which columns go into X and y.","metadata":{}},{"cell_type":"code","source":"X = df.drop('customer sentiment', axis = 1)","metadata":{"id":"i46FM8sgGwy_","execution":{"iopub.status.busy":"2022-08-15T08:47:34.645258Z","iopub.execute_input":"2022-08-15T08:47:34.645675Z","iopub.status.idle":"2022-08-15T08:47:34.654092Z","shell.execute_reply.started":"2022-08-15T08:47:34.645639Z","shell.execute_reply":"2022-08-15T08:47:34.652818Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"y = df['customer sentiment']","metadata":{"id":"4OuZ_hL9Gz5A","execution":{"iopub.status.busy":"2022-08-15T08:47:34.655468Z","iopub.execute_input":"2022-08-15T08:47:34.655861Z","iopub.status.idle":"2022-08-15T08:47:34.666412Z","shell.execute_reply.started":"2022-08-15T08:47:34.655828Z","shell.execute_reply":"2022-08-15T08:47:34.665521Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"markdown","source":"**Split the train and test data**","metadata":{"id":"7FTXWtQuCKQg"}},{"cell_type":"markdown","source":"As usual, weâ€™ll be splitting our data into train and test subsets while ensuring that the resulting split is stratified.","metadata":{"id":"1hw9UDCvLNMb"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"id":"HQI124FvHQDk","execution":{"iopub.status.busy":"2022-08-15T08:47:34.669951Z","iopub.execute_input":"2022-08-15T08:47:34.670626Z","iopub.status.idle":"2022-08-15T08:47:34.679084Z","shell.execute_reply.started":"2022-08-15T08:47:34.670586Z","shell.execute_reply":"2022-08-15T08:47:34.678068Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)","metadata":{"id":"U-IQFV85HSx1","execution":{"iopub.status.busy":"2022-08-15T08:47:34.681331Z","iopub.execute_input":"2022-08-15T08:47:34.682209Z","iopub.status.idle":"2022-08-15T08:47:34.710418Z","shell.execute_reply.started":"2022-08-15T08:47:34.682159Z","shell.execute_reply":"2022-08-15T08:47:34.709070Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"markdown","source":"**Create a model pipeline**","metadata":{}},{"cell_type":"markdown","source":"1. The first step in this pipeline is to use a SimpleImputer to fill in the missing values (np.NaN) with \"missing\". Although there are many other strategies to use when filling in missing values, there could be underlying reasons in the data collection why an observation has missing data. Therefore, to simply fill in the missing values with the most_frequent of the data would be adding bias from us, the researcher. Without knowing more about why these values are np.nan, we can just fill in the value with \"missing\" for categorical features.\n\n2. We then pipe this into a OneHotEncoder in order to encode each categorical variable's values as a separate binary column.\n\n3. Next, we use a RobustScalar to normalize our non-textual data.\n\n4. Finally, we need to convert our text to a numeric form. Machine learning models canâ€™t use text, so the final step is to use a text preprocessing technique called Count Vectorization to turn the text into a vector of numbers via the CountVectorizer module in scikit-learn.","metadata":{}},{"cell_type":"code","source":"import time\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import RobustScaler\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import f1_score","metadata":{"id":"PowS3pusqaAy","execution":{"iopub.status.busy":"2022-08-15T08:47:34.712258Z","iopub.execute_input":"2022-08-15T08:47:34.712695Z","iopub.status.idle":"2022-08-15T08:47:34.720263Z","shell.execute_reply.started":"2022-08-15T08:47:34.712655Z","shell.execute_reply":"2022-08-15T08:47:34.718859Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"def get_pipeline(X, model):\n  \n    categorical_columns = list(X.select_dtypes(include = ['object']).columns.values.tolist())\n    categorical_columns.remove('all text')\n    \n    numeric_columns = list(X.select_dtypes(exclude = ['object']).columns.values.tolist()) \n    \n    categorical_transformer = Pipeline(steps = [('simple_imputer', SimpleImputer(missing_values = np.nan, fill_value = 'missing', strategy = 'constant')),\n                                                ('one_hot_encoder', OneHotEncoder(sparse = False, handle_unknown = 'ignore')),\n                                                ('scaler', RobustScaler())])    \n     \n    preprocessor = ColumnTransformer(transformers = [('text', CountVectorizer(), 'all text'),\n                                                     ('categorical', categorical_transformer, categorical_columns),\n                                                     ('numeric', RobustScaler(), numeric_columns)], remainder = 'passthrough')\n\n    bundled_pipeline = Pipeline(steps = [('preprocessor', preprocessor),\n                                         ('model', model)])\n    \n    return bundled_pipeline","metadata":{"id":"tm7n1QuJTe1X","execution":{"iopub.status.busy":"2022-08-15T08:47:34.722275Z","iopub.execute_input":"2022-08-15T08:47:34.722966Z","iopub.status.idle":"2022-08-15T08:47:34.733732Z","shell.execute_reply.started":"2022-08-15T08:47:34.722911Z","shell.execute_reply":"2022-08-15T08:47:34.732545Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"markdown","source":"**Apply model selection**","metadata":{"id":"uIEHoW7aCYlo"}},{"cell_type":"markdown","source":"To undertake the model selection step, we first need to create a dictionary containing the name of each model we want to test, and the name of the model class, i.e. XGBClassifier(random_state = 42).\n\nNext weâ€™ll create a Pandas dataframe into which weâ€™ll store the data. Then weâ€™ll loop over each of the models, fit it using the X_train and y_train data, then generate predictions from X_test and calculate the mean F1 score from 5 rounds of cross-validation. That will give us the F1 score for the X_test data, plus the average F1 score for the training data set.","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\nfrom sklearn.ensemble import BaggingClassifier, AdaBoostClassifier","metadata":{"id":"VhwkNA6OIB0r","execution":{"iopub.status.busy":"2022-08-15T08:47:34.735374Z","iopub.execute_input":"2022-08-15T08:47:34.736164Z","iopub.status.idle":"2022-08-15T08:47:34.750229Z","shell.execute_reply.started":"2022-08-15T08:47:34.736123Z","shell.execute_reply":"2022-08-15T08:47:34.749096Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"def select_model(X, y, pipeline = None):\n\n  classifiers = {}\n  classifiers.update({'XGBClassifier': OneVsRestClassifier(XGBClassifier(random_state = 42))})\n  classifiers.update({'LGBMClassifier': OneVsRestClassifier(LGBMClassifier(random_state = 42))})\n  classifiers.update({'DecisionTreeClassifier': OneVsRestClassifier(DecisionTreeClassifier(random_state = 42))})\n  classifiers.update({'RandomForestClassifier': OneVsRestClassifier(RandomForestClassifier(random_state = 42))})\n  classifiers.update({'ExtraTreesClassifier': OneVsRestClassifier(ExtraTreesClassifier(random_state = 42))})\n  classifiers.update({'GradientBoostingClassifier': OneVsRestClassifier(GradientBoostingClassifier(random_state = 42))})    \n  classifiers.update({'BaggingClassifier': OneVsRestClassifier(BaggingClassifier(random_state = 42))})\n  classifiers.update({'AdaBoostClassifier': OneVsRestClassifier(AdaBoostClassifier(random_state = 42))})\n  \n\n  df_models = pd.DataFrame(columns = ['model', 'run_time', 'f1_score_cv', 'f1_score'])\n\n  for key in classifiers:\n\n      print('*', key)\n\n      start_time = time.time()\n      \n      pipeline = get_pipeline(X_train, classifiers[key])\n\n      cv = cross_val_score(pipeline, X, y, cv = 5, scoring = 'f1_macro', n_jobs = -1)\n      \n      pipeline.fit(X_train, y_train)\n      y_pred = pipeline.predict(X_test)\n    \n      row = {'model': key,\n             'run_time': format(round((time.time() - start_time) / 60, 2)),\n             'f1_score_cv': cv.mean(),\n             'f1_score': f1_score(y_test, y_pred, average = 'macro')}\n      \n      df_models = df_models.append(row, ignore_index = True)\n\n  df_models = df_models.sort_values(by = 'f1_score', ascending = False)\n      \n  return df_models","metadata":{"id":"PzGT_8mzIi35","execution":{"iopub.status.busy":"2022-08-15T08:47:34.752057Z","iopub.execute_input":"2022-08-15T08:47:34.752815Z","iopub.status.idle":"2022-08-15T08:47:34.766427Z","shell.execute_reply.started":"2022-08-15T08:47:34.752774Z","shell.execute_reply":"2022-08-15T08:47:34.765259Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"models = select_model(X_train, y_train)","metadata":{"id":"f8kiXDjrc5FT","outputId":"89d73912-ac83-4ab0-880c-8a7dec7847e2","execution":{"iopub.status.busy":"2022-08-15T08:47:34.769966Z","iopub.execute_input":"2022-08-15T08:47:34.770966Z","iopub.status.idle":"2022-08-15T08:52:55.811875Z","shell.execute_reply.started":"2022-08-15T08:47:34.770922Z","shell.execute_reply":"2022-08-15T08:52:55.810407Z"},"trusted":true},"execution_count":123,"outputs":[{"name":"stdout","text":"* XGBClassifier\n* LGBMClassifier\n* DecisionTreeClassifier\n* RandomForestClassifier\n* ExtraTreesClassifier\n* GradientBoostingClassifier\n* BaggingClassifier\n* AdaBoostClassifier\n","output_type":"stream"}]},{"cell_type":"markdown","source":"After 5-6 minutes, the model selection process had completed. This identified that XGBClassifier was the top performing model, with an F1 score of 100%.","metadata":{"id":"v2YsgZCNCg6j"}},{"cell_type":"code","source":"models.head(10)","metadata":{"id":"ZIokc2CXfc4N","outputId":"2a366564-3da2-4548-fa1e-1bf43927b115","execution":{"iopub.status.busy":"2022-08-15T08:52:55.814283Z","iopub.execute_input":"2022-08-15T08:52:55.814682Z","iopub.status.idle":"2022-08-15T08:52:55.830889Z","shell.execute_reply.started":"2022-08-15T08:52:55.814637Z","shell.execute_reply":"2022-08-15T08:52:55.829159Z"},"trusted":true},"execution_count":124,"outputs":[{"execution_count":124,"output_type":"execute_result","data":{"text/plain":"                        model run_time  f1_score_cv  f1_score\n0               XGBClassifier     0.42     1.000000  1.000000\n1              LGBMClassifier     0.24     1.000000  1.000000\n2      DecisionTreeClassifier     0.05     1.000000  1.000000\n5  GradientBoostingClassifier     0.66     1.000000  1.000000\n6           BaggingClassifier     0.12     1.000000  1.000000\n7          AdaBoostClassifier     0.17     1.000000  1.000000\n3      RandomForestClassifier     1.41     0.966691  0.951954\n4        ExtraTreesClassifier     2.27     0.872257  0.863240","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>run_time</th>\n      <th>f1_score_cv</th>\n      <th>f1_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>XGBClassifier</td>\n      <td>0.42</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>LGBMClassifier</td>\n      <td>0.24</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>DecisionTreeClassifier</td>\n      <td>0.05</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>GradientBoostingClassifier</td>\n      <td>0.66</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>BaggingClassifier</td>\n      <td>0.12</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>AdaBoostClassifier</td>\n      <td>0.17</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>RandomForestClassifier</td>\n      <td>1.41</td>\n      <td>0.966691</td>\n      <td>0.951954</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ExtraTreesClassifier</td>\n      <td>2.27</td>\n      <td>0.872257</td>\n      <td>0.863240</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"**Examine the performance of the best model**","metadata":{"id":"KV2tXEEDg6LN"}},{"cell_type":"code","source":"bundled_pipeline = get_pipeline(X_train, OneVsRestClassifier(XGBClassifier(random_state = 42)))\nbundled_pipeline.fit(X_train, y_train)\ny_pred = bundled_pipeline.predict(X_test)","metadata":{"id":"3BwP-91r0-Vz","execution":{"iopub.status.busy":"2022-08-15T08:52:55.832725Z","iopub.execute_input":"2022-08-15T08:52:55.833763Z","iopub.status.idle":"2022-08-15T08:53:01.208003Z","shell.execute_reply.started":"2022-08-15T08:52:55.833712Z","shell.execute_reply":"2022-08-15T08:53:01.206978Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"markdown","source":"Since this is a multi-class classification problem, we can assess its performance using common classification metrics, such as the F1 score and classification report. \n","metadata":{"id":"C2_3vaBehGVZ"}},{"cell_type":"code","source":"from sklearn.metrics import classification_report","metadata":{"id":"M-tuwkETiFoL","execution":{"iopub.status.busy":"2022-08-15T08:53:01.209695Z","iopub.execute_input":"2022-08-15T08:53:01.213640Z","iopub.status.idle":"2022-08-15T08:53:01.220675Z","shell.execute_reply.started":"2022-08-15T08:53:01.213583Z","shell.execute_reply":"2022-08-15T08:53:01.219667Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred))","metadata":{"id":"t1EJtG9Ih_F4","outputId":"cf98a032-ae97-4133-a226-112ebce160ab","execution":{"iopub.status.busy":"2022-08-15T08:53:01.224571Z","iopub.execute_input":"2022-08-15T08:53:01.225842Z","iopub.status.idle":"2022-08-15T08:53:01.248212Z","shell.execute_reply.started":"2022-08-15T08:53:01.225792Z","shell.execute_reply":"2022-08-15T08:53:01.246929Z"},"trusted":true},"execution_count":127,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n          -1       1.00      1.00      1.00       482\n           0       1.00      1.00      1.00       574\n           1       1.00      1.00      1.00      3642\n\n    accuracy                           1.00      4698\n   macro avg       1.00      1.00      1.00      4698\nweighted avg       1.00      1.00      1.00      4698\n\n","output_type":"stream"}]}]}